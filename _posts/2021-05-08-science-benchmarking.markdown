---
layout: post
title:  "Benchmarking an AI scientist"
date:   2021-05-08 05:09:15 +0000
categories: ai science rl benchmarking
---
Physical theories are pretty incredible when you think about what they're doing from a machine learning perspective. With just a few equations - a few parameters - they make predictions about the behavior of an infinite number of configurations of stuff.

Most machine learning models do something very different. Instead of a few equations, they have millions of parameters. And instead of making predictions about an infinite number of configurations, they focus on a large but finite subset of configurations.

One of the frustrating aspects of the current generation of ML models is that in most applications, sooner or later your model bumps into a part of configuration space that it doesn't know anything about - and then it fails spectacularly. During training, the model absorbs information and stores it in the model parameters. After a finite amount of training, the model has only stored a finite amount of information. This seems pretty reasonable. How could it possibly know about an infinite number of configurations? If your model is failing somewhere, the thinking goes, that means you need more training data to cover the spot where it's failing. But while this line of thinking is perfectly reasonable, it has some real drawbacks.

For instance, think of self-driving cars. That task turned out to be substantially more difficult than anticipated. The challenge is what's called the "long tail" of driving scenarios. Although perhaps 99% of driving situations are common and therefore easy for the ML system to learn to handle through experience, the other 1% comprise a bewildering array of special cases that are, individually, very rare. A refrigerator left in the middle of a road, a parade of protestors, an oversized truck that takes up 1.5 lanes: have you ever actually encountered these things before? But could you if you encountered them for the first time? The ML system never has a chance to see each of them often enough during training to learn to deal with them properly. In practice, of course, as you train your system with more and more examples, you may reach a point where that 1% figure is reduced to 0.0001%. If the question is simply the probability of an accident per mile, then eventually that self-driving car may be ready to use. Even so, it is unnerving to know that in certain rare cases, the system may totally fail in a way that is incomprehensible to humans. And a huge amount of cost could have been avoided if the system could have generalized to parts of configuration space that it didn't have direct access to during training.

One of the challenges of creating better ML systems - I think you could call it a "grand challenge" - is making systems that cover a much larger set of scenarios than the ones they've been trained on.

To a traditional ML practitioner's ear, that's nonsense. How can a statistical learning system handle cases it hasn't seen during training?

Amazingly, physical theories do exactly that. It isn't magic. The reason physical theories can do this is because the universe we live in is extremely weird, and weird in a way that physical theories exploit but that ML models typically haven't exploited. Namely, the universe is made of many, many copies of the same kinds of basic entities, and the entities interact according to a small set of rules. The most successful physical theories rely heavily on these ideas, while the vast majority of ML models don't.

This basic property of our universe explains how you can learn about an infinite variety of scenarios from only a finite number of observations: you only need a finite number of observations to learn about the basic types of entities and the rules that govern how they interact. Once you know that, you can make predictions about a particular configuration by simply applying the rules to all the entities in that configuration. The entity types and their interaction rules are finite; the range of configurations is infinite, but each configuration behaves predictably.

Yoshua Bengio, Peter Battaglia, Jessica Hamrick, Joshua Tenenbaum, Bernhard Skolkopf and other ML researchers have been taking this line of thinking seriously, and I think we need to pay even more attention to it.

In order to make progress on this type of work, we are going to think very carefully about the types of benchmark tasks we use. That's because, from the argument I made above about why physical theories have predictive reach far beyond the scenarios on which they were trained, we can see that it's not possible in all worlds: it's only possible in worlds made of repetitive interacting entities. That's the essence of why ML hasn't made much progress in this direction: it's focused on other types of worlds (which are good approximations to our world in many specific applications of interest, but not in general).

Atari games provide a great example of the problem. Atari has been an extremely influential benchmark in RL in the past 5 years. But if you think about it, Atari games don't have the entities-and-interactions structure that makes physical theories possible. Therefore, we might conjecture, they also lack the structure needed to allow ML models to generalize beyond the scenarios on which they were trained.

It's helpful to think in terms of information content. In Atari games, or other small complex puzzles, the amount of information is relatively small: it's possible for a machine learning model to absorb virtually all the information during training. Furthermore, often the information cannot be reduced to a basic set of entities interacting via a finite set of rules: rather, the complexity is hard-coded into the game by programmers. No matter how much you play Donkey Kong, you cannot discover a physical theory like Newtonian mechanics (or even an analogous theory), because the world of Donkey Kong does not emerge from simple interacting entities in the way that our world does. I'm oversimplifying somewhat: simple video games do sometimes involve interactions between simple entities. However, those interactions and entities are recombined in an extremely limited fashion - therefore it's been possible for ML systems to achieve state-of-the-art performance by simply absorbing all the information of the game without developing an accurate model of the entities and interactions.

Benchmarks matter: they drive the creation of new algorithms. To drive the creation of machine learning systems that do more than simply memorize a finite volume of configuration space, we need to use benchmark environments in which: a) the tasks emerge from the interactions of basic entities, and b) the task is structured so that memorizing a finite volume of configuration space is not a viable strategy for solving the problem.


